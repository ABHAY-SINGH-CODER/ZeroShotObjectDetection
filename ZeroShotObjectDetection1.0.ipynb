{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "da7c69bb-4aef-499e-9faa-aef0e5776c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt Analysis] Category: high, Thresholds: [(0.3, 0.4), (0.35, 0.45), (0.25, 0.5)]\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n",
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_0_b0.30_t0.40] CLIP score = 0.2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_1_b0.35_t0.45] CLIP score = 0.2010\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_2_b0.25_t0.50] CLIP score = 0.2010\n",
      "\n",
      "======================\n",
      "Best thresholds: box_th=0.30, text_th=0.40\n",
      "Best CLIP score: 0.2010\n",
      "Annotated: threshold_runs/run_0_b0.30_t0.40_annot.png\n",
      "Mask:      threshold_runs/run_0_b0.30_t0.40_mask.png\n",
      "\n",
      "Annotating original image with the best result...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final annotation by overwriting: 'test.png'\n",
      "\n",
      "âœ… Done! All runs saved in 'threshold_runs/'. Best copies: 'best_annotated.png', 'best_mask.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    }
   ],
   "source": [
    "# =======================\n",
    "# CPU-Only GroundingDINO + DBSCAN + SAM\n",
    "# Multi-threshold search scored by CLIP similarity\n",
    "# =======================\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "import torch, cv2, numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import re\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "# -------- SETTINGS --------\n",
    "IMAGE_PATH = \"test.png\"\n",
    "TEXT_PROMPT = \"wall with paints\"\n",
    "DEVICE = \"cpu\"\n",
    "\n",
    "# ===== Added: Prompt Analysis for Threshold Selection =====\n",
    "THRESHOLD_PRESETS = {\n",
    "    \"low\":    [(0.05, 0.25), (0.10, 0.28), (0.20, 0.30)],  # for many items\n",
    "    \"medium\": [(0.15, 0.32), (0.15, 0.35), (0.18, 0.38)],  # for average prompts\n",
    "    \"high\":   [(0.30, 0.40), (0.35, 0.45), (0.25, 0.50)]   # for very specific\n",
    "}\n",
    "\n",
    "def analyze_prompt(prompt: str):\n",
    "    prompt_clean = prompt.lower().strip()\n",
    "    objects = re.split(r',| and ', prompt_clean)\n",
    "    objects = [obj.strip() for obj in objects if obj.strip()]\n",
    "    num_objects = len(objects)\n",
    "\n",
    "    if num_objects >= 3:\n",
    "        category = \"low\"\n",
    "    elif num_objects == 2:\n",
    "        category = \"medium\"\n",
    "    else:\n",
    "        category = \"high\"\n",
    "\n",
    "    return category, THRESHOLD_PRESETS[category]\n",
    "\n",
    "# Use analyzed thresholds\n",
    "category, THRESHOLD_PAIRS = analyze_prompt(TEXT_PROMPT)\n",
    "print(f\"[Prompt Analysis] Category: {category}, Thresholds: {THRESHOLD_PAIRS}\")\n",
    "# ===========================================================\n",
    "\n",
    "# Load GroundingDINO\n",
    "dino_config_path = \"groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "dino_weights_path = \"weights/groundingdino_swint_ogc copy.pth\"\n",
    "dino_model = load_model(dino_config_path, dino_weights_path)\n",
    "dino_model.to(DEVICE).eval()\n",
    "\n",
    "# Load image once\n",
    "image_source, image = load_image(IMAGE_PATH)  # image_source: RGB np.ndarray\n",
    "H, W = image_source.shape[:2]\n",
    "\n",
    "# Load SAM\n",
    "sam_checkpoint = \"weights/sam_vit_h_4b8939.pth\"\n",
    "sam_model = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
    "sam_model.to(DEVICE)\n",
    "sam_predictor = SamPredictor(sam_model)\n",
    "sam_predictor.set_image(image_source)\n",
    "\n",
    "# Load CLIP\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "clip_model.to(DEVICE).eval()\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "# Output dir\n",
    "OUTDIR = \"threshold_runs\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)\n",
    "\n",
    "def dbscan_merge_to_single_box(boxes_xyxy_np, eps_frac=0.05):\n",
    "    \"\"\"Merge all boxes into a single one after optional DBSCAN clustering.\"\"\"\n",
    "    if len(boxes_xyxy_np) == 0:\n",
    "        return None\n",
    "    # Cluster centers\n",
    "    centers = np.column_stack((\n",
    "        (boxes_xyxy_np[:, 0] + boxes_xyxy_np[:, 2]) / 2.0,  # cx\n",
    "        (boxes_xyxy_np[:, 1] + boxes_xyxy_np[:, 3]) / 2.0   # cy\n",
    "    ))\n",
    "    eps = eps_frac * max(W, H)\n",
    "    _ = DBSCAN(eps=eps, min_samples=1).fit(centers)  # not using clusters, just merging all\n",
    "\n",
    "    # Merge all into one\n",
    "    x_min = boxes_xyxy_np[:, 0].min()\n",
    "    y_min = boxes_xyxy_np[:, 1].min()\n",
    "    x_max = boxes_xyxy_np[:, 2].max()\n",
    "    y_max = boxes_xyxy_np[:, 3].max()\n",
    "    return np.array([[x_min, y_min, x_max, y_max]], dtype=np.float32)\n",
    "\n",
    "def clip_similarity_score(image_path: str, prompt: str) -> float:\n",
    "    \"\"\"Get CLIP cosine similarity between image and prompt.\"\"\"\n",
    "    img_clip = clip_preprocess(Image.open(image_path)).unsqueeze(0).to(DEVICE)\n",
    "    text_clip = tokenizer([prompt]).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        img_feat = clip_model.encode_image(img_clip)\n",
    "        txt_feat = clip_model.encode_text(text_clip)\n",
    "        img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "        txt_feat /= txt_feat.norm(dim=-1, keepdim=True)\n",
    "        similarity = (img_feat @ txt_feat.T).item()\n",
    "    return similarity\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, (box_th, text_th) in enumerate(THRESHOLD_PAIRS):\n",
    "    run_tag = f\"run_{i}_b{box_th:.2f}_t{text_th:.2f}\"\n",
    "\n",
    "    # ---- GroundingDINO detection\n",
    "    boxes_t, logits_t, phrases = predict(\n",
    "        model=dino_model,\n",
    "        image=image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=box_th,\n",
    "        text_threshold=text_th,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if len(boxes_t) == 0:\n",
    "        results.append({\"score\": -1.0, \"pair\": (box_th, text_th), \"annot_path\": None, \"mask_path\": None})\n",
    "        print(f\"[{run_tag}] No detections -> CLIP score = -1\")\n",
    "        continue\n",
    "\n",
    "    # ---- Merge boxes to one\n",
    "    boxes_np = boxes_t.detach().cpu().numpy().astype(np.float32)\n",
    "    merged_np = dbscan_merge_to_single_box(boxes_np, eps_frac=0.05)\n",
    "    merged_t = torch.from_numpy(merged_np)\n",
    "\n",
    "    # ---- Annotate merged box\n",
    "    annot_img = annotate(image_source, merged_t, logits=torch.ones((1,)), phrases=[TEXT_PROMPT])\n",
    "    annot_path = os.path.join(OUTDIR, f\"{run_tag}_annot.png\")\n",
    "    cv2.imwrite(annot_path, cv2.cvtColor(annot_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # ---- SAM segmentation\n",
    "    image_bgr = cv2.cvtColor(image_source, cv2.COLOR_RGB2BGR)\n",
    "    masks, _, _ = sam_predictor.predict(box=merged_np[0], multimask_output=False)\n",
    "    mask_img = image_bgr.copy()\n",
    "    mask_img[masks[0]] = (0, 255, 0)  # green overlay\n",
    "    mask_path = os.path.join(OUTDIR, f\"{run_tag}_mask.png\")\n",
    "    cv2.imwrite(mask_path, mask_img)\n",
    "\n",
    "    # ---- CLIP scoring\n",
    "    score = clip_similarity_score(annot_path, TEXT_PROMPT)\n",
    "    results.append({\"score\": score, \"pair\": (box_th, text_th), \"annot_path\": annot_path, \"mask_path\": mask_path})\n",
    "    print(f\"[{run_tag}] CLIP score = {score:.4f}\")\n",
    "\n",
    "# ---- Pick best by CLIP score\n",
    "best = max(results, key=lambda r: r[\"score\"])\n",
    "print(\"\\n======================\")\n",
    "print(f\"Best thresholds: box_th={best['pair'][0]:.2f}, text_th={best['pair'][1]:.2f}\")\n",
    "print(f\"Best CLIP score: {best['score']:.4f}\")\n",
    "print(f\"Annotated: {best['annot_path']}\")\n",
    "print(f\"Mask:      {best['mask_path']}\")\n",
    "\n",
    "# Copy best to top-level for convenience\n",
    "if best[\"annot_path\"]:\n",
    "    shutil.copy(best[\"annot_path\"], \"best_annotated.png\")\n",
    "if best[\"mask_path\"]:\n",
    "    shutil.copy(best[\"mask_path\"], \"best_mask.png\")\n",
    "\n",
    "# =================================================================================\n",
    "# ---- ADDED: Annotate the original image with the best bounding box found ----\n",
    "if best[\"score\"] > -1.0:\n",
    "    print(\"\\nAnnotating original image with the best result...\")\n",
    "    \n",
    "    # Re-run detection and merging with the best thresholds to get the final box\n",
    "    best_box_th, best_text_th = best['pair']\n",
    "    final_boxes_t, _, _ = predict(\n",
    "        model=dino_model,\n",
    "        image=image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=best_box_th,\n",
    "        text_threshold=best_text_th,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    if len(final_boxes_t) > 0:\n",
    "        final_boxes_np = final_boxes_t.cpu().numpy()\n",
    "        final_merged_np = dbscan_merge_to_single_box(final_boxes_np)\n",
    "        final_merged_t = torch.from_numpy(final_merged_np)\n",
    "\n",
    "        # Use the original image_source for annotation\n",
    "        final_annotated_img = annotate(\n",
    "            image_source=image_source.copy(), \n",
    "            boxes=final_merged_t, \n",
    "            logits=torch.ones(final_merged_t.shape[0]), \n",
    "            phrases=[TEXT_PROMPT]\n",
    "        )\n",
    "        \n",
    "        # Save the final annotated original image by overwriting it\n",
    "        final_output_path = IMAGE_PATH\n",
    "        cv2.imwrite(final_output_path, cv2.cvtColor(final_annotated_img, cv2.COLOR_RGB2BGR))  # Fixed!\n",
    "        print(f\"Saved final annotation by overwriting: '{final_output_path}'\")\n",
    "    else:\n",
    "        print(\"Could not generate final annotation as no boxes were detected with best thresholds.\")\n",
    "\n",
    "\n",
    "print(\"\\nâœ… Done! All runs saved in 'threshold_runs/'. Best copies: 'best_annotated.png', 'best_mask.png'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c253f67-925f-4618-9f56-417f493fb770",
   "metadata": {},
   "source": [
    "# CPU-Based Object Detection with GroundingDINO and SAM\n",
    "\n",
    "This notebook demonstrates how to set up and run **GroundingDINO** combined with **Segment Anything Model (SAM)** on a CPU, using a text prompt to detect objects in an image.\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸ Import Required Libraries\n",
    "\n",
    "```python\n",
    "import os\n",
    "import shutil\n",
    "import torch, cv2, numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import re\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from segment_anything import sam_model_registry, SamPredictor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "99dd3a5c-462e-4018-af4d-458899803926",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch, cv2, numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "from PIL import Image\n",
    "import open_clip\n",
    "import re\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate\n",
    "from segment_anything import sam_model_registry, SamPredictor\n",
    "\n",
    "\n",
    "IMAGE_PATH = \"test.png\"\n",
    "TEXT_PROMPT = \"wall with paints\"\n",
    "DEVICE = \"cpu\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8d3a10d-16bb-43ea-a635-773c1ae3b739",
   "metadata": {},
   "source": [
    "# Prompt Analysis and Threshold Selection for Object Detection\n",
    "\n",
    "This section adds **adaptive threshold selection** based on the prompt complexity. It also includes **DBSCAN-based merging** of multiple bounding boxes and **CLIP similarity scoring**.\n",
    "\n",
    "---\n",
    "\n",
    "- Calculates center coordinates $(cx, cy)$ of each bounding box.\n",
    "- Clusters box centers with DBSCAN (optional, mostly to merge close boxes).\n",
    "- Merges all boxes into one bounding box by taking min/max coordinates.\n",
    "\n",
    "Formula for center coordinates:\n",
    "\n",
    "$$\n",
    "cx = \\frac{x_{min} + x_{max}}{2}, \\quad cy = \\frac{y_{min} + y_{max}}{2}\n",
    "$$\n",
    "\n",
    "Merged box coordinates:\n",
    "\n",
    "$$\n",
    "x_{min}^{merged} = \\min(x_{min}), \\quad y_{min}^{merged} = \\min(y_{min}), \\quad x_{max}^{merged} = \\max(x_{max}), \\quad y_{max}^{merged} = \\max(y_{max})\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "- Preprocess image and prompt for CLIP.\n",
    "- Encode image and text to feature vectors.\n",
    "- Normalize vectors and calculate cosine similarity:\n",
    "\n",
    "$$\n",
    "\\text{similarity} = \\frac{\\text{img\\_feat} \\cdot \\text{txt\\_feat}}{\\|\\text{img\\_feat}\\| \\|\\text{txt\\_feat}\\|}\n",
    "$$\n",
    "\n",
    "Returns a score between -1 and 1 indicating how well the image matches the prompt.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "852fdb78-835b-4f88-8940-d1bdacd8fab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===== Added: Prompt Analysis for Threshold Selection =====\n",
    "THRESHOLD_PRESETS = {\n",
    "    \"low\":    [(0.05, 0.25), (0.10, 0.28), (0.20, 0.30)],  # for many items\n",
    "    \"medium\": [(0.15, 0.32), (0.15, 0.35), (0.18, 0.38)],  # for average prompts\n",
    "    \"high\":   [(0.30, 0.40), (0.35, 0.45), (0.25, 0.50)]   # for very specific\n",
    "}\n",
    "\n",
    "def analyze_prompt(prompt: str):\n",
    "    prompt_clean = prompt.lower().strip()\n",
    "    objects = re.split(r',| and ', prompt_clean)\n",
    "    objects = [obj.strip() for obj in objects if obj.strip()]\n",
    "    num_objects = len(objects)\n",
    "\n",
    "    if num_objects >= 3:\n",
    "        category = \"low\"\n",
    "    elif num_objects == 2:\n",
    "        category = \"medium\"\n",
    "    else:\n",
    "        category = \"high\"\n",
    "\n",
    "    return category, THRESHOLD_PRESETS[category]\n",
    "\n",
    "def dbscan_merge_to_single_box(boxes_xyxy_np, eps_frac=0.05):\n",
    "    \"\"\"Merge all boxes into a single one after optional DBSCAN clustering.\"\"\"\n",
    "    if len(boxes_xyxy_np) == 0:\n",
    "        return None\n",
    "    # Cluster centers\n",
    "    centers = np.column_stack((\n",
    "        (boxes_xyxy_np[:, 0] + boxes_xyxy_np[:, 2]) / 2.0,  # cx\n",
    "        (boxes_xyxy_np[:, 1] + boxes_xyxy_np[:, 3]) / 2.0   # cy\n",
    "    ))\n",
    "    # Note: W and H are accessed from the global scope here\n",
    "    eps = eps_frac * max(W, H)\n",
    "    _ = DBSCAN(eps=eps, min_samples=1).fit(centers)  # not using clusters, just merging all\n",
    "\n",
    "    # Merge all into one\n",
    "    x_min = boxes_xyxy_np[:, 0].min()\n",
    "    y_min = boxes_xyxy_np[:, 1].min()\n",
    "    x_max = boxes_xyxy_np[:, 2].max()\n",
    "    y_max = boxes_xyxy_np[:, 3].max()\n",
    "    return np.array([[x_min, y_min, x_max, y_max]], dtype=np.float32)\n",
    "\n",
    "def clip_similarity_score(image_path: str, prompt: str) -> float:\n",
    "    \"\"\"Get CLIP cosine similarity between image and prompt.\"\"\"\n",
    "    img_clip = clip_preprocess(Image.open(image_path)).unsqueeze(0).to(DEVICE)\n",
    "    text_clip = tokenizer([prompt]).to(DEVICE)\n",
    "    with torch.no_grad():\n",
    "        img_feat = clip_model.encode_image(img_clip)\n",
    "        txt_feat = clip_model.encode_text(text_clip)\n",
    "        img_feat /= img_feat.norm(dim=-1, keepdim=True)\n",
    "        txt_feat /= txt_feat.norm(dim=-1, keepdim=True)\n",
    "        similarity = (img_feat @ txt_feat.T).item()\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbc86359-2022-4dea-992a-7c942652a14b",
   "metadata": {},
   "source": [
    "# Setup: Load Models, Image, and Thresholds\n",
    "\n",
    "This section demonstrates **loading all required models**, performing **prompt analysis**, and setting up directories for output.\n",
    "\n",
    "-------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "29d75f3a-3cd6-4973-ae62-cc8a9b55ace2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Prompt Analysis] Category: high, Thresholds: [(0.3, 0.4), (0.35, 0.45), (0.25, 0.5)]\n",
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: QuickGELU mismatch between final model config (quick_gelu=False) and pretrained tag 'openai' (quick_gelu=True).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "category, THRESHOLD_PAIRS = analyze_prompt(TEXT_PROMPT)\n",
    "print(f\"[Prompt Analysis] Category: {category}, Thresholds: {THRESHOLD_PAIRS}\")\n",
    "\n",
    "# Load GroundingDINO\n",
    "dino_config_path = \"groundingdino/config/GroundingDINO_SwinT_OGC.py\"\n",
    "dino_weights_path = \"weights/groundingdino_swint_ogc copy.pth\"\n",
    "dino_model = load_model(dino_config_path, dino_weights_path)\n",
    "dino_model.to(DEVICE).eval()\n",
    "\n",
    "# Load image once\n",
    "image_source, image = load_image(IMAGE_PATH)  # image_source: RGB np.ndarray\n",
    "H, W = image_source.shape[:2]\n",
    "\n",
    "# Load SAM\n",
    "sam_checkpoint = \"weights/sam_vit_h_4b8939.pth\"\n",
    "sam_model = sam_model_registry[\"vit_h\"](checkpoint=sam_checkpoint)\n",
    "sam_model.to(DEVICE)\n",
    "sam_predictor = SamPredictor(sam_model)\n",
    "sam_predictor.set_image(image_source)\n",
    "\n",
    "# Load CLIP\n",
    "clip_model, _, clip_preprocess = open_clip.create_model_and_transforms(\"ViT-B-32\", pretrained=\"openai\")\n",
    "clip_model.to(DEVICE).eval()\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "\n",
    "# Output dir\n",
    "OUTDIR = \"threshold_runs\"\n",
    "os.makedirs(OUTDIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721849e-bb0f-4308-9db8-4f5fcd08e214",
   "metadata": {},
   "source": [
    "## ðŸ§  Workflow Overview: Detection, Segmentation & Scoring\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 1: Iterate Over Threshold Pairs\n",
    "- Loop through each `(box_threshold, text_threshold)` in `THRESHOLD_PAIRS`.\n",
    "- Create a unique `run_tag` for each configuration.\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 2: GroundingDINO Detection\n",
    "- Use `predict()` to detect bounding boxes and phrases:\n",
    "  - Inputs: image, text prompt, thresholds.\n",
    "  - Outputs: `boxes_t`, `logits_t`, `phrases`.\n",
    "- If no boxes are detected:\n",
    "  - Append result with score `-1.0`.\n",
    "  - Skip to next iteration.\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 3: Merge Boxes\n",
    "- Convert `boxes_t` to NumPy array.\n",
    "- Merge close boxes using `dbscan_merge_to_single_box()`.\n",
    "\n",
    "---\n",
    "\n",
    "###  Step 4: Annotate Merged Box\n",
    "- Use `annotate()` to draw merged box and label.\n",
    "- Save annotated image to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "2bed5d2e-b5a9-46de-b3bd-867de2bdf261",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n",
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run_0_b0.30_t0.40] No detections -> CLIP score = -1\n",
      "[run_1_b0.35_t0.45] No detections -> CLIP score = -1\n",
      "[run_2_b0.25_t0.50] CLIP score = 0.2021\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for i, (box_th, text_th) in enumerate(THRESHOLD_PAIRS):\n",
    "    run_tag = f\"run_{i}_b{box_th:.2f}_t{text_th:.2f}\"\n",
    "\n",
    "    # ---- GroundingDINO detection\n",
    "    boxes_t, logits_t, phrases = predict(\n",
    "        model=dino_model,\n",
    "        image=image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=box_th,\n",
    "        text_threshold=text_th,\n",
    "        device=DEVICE\n",
    "    )\n",
    "\n",
    "    if len(boxes_t) == 0:\n",
    "        results.append({\"score\": -1.0, \"pair\": (box_th, text_th), \"annot_path\": None, \"mask_path\": None})\n",
    "        print(f\"[{run_tag}] No detections -> CLIP score = -1\")\n",
    "        continue\n",
    "\n",
    "    # ---- Merge boxes to one\n",
    "    boxes_np = boxes_t.detach().cpu().numpy().astype(np.float32)\n",
    "    merged_np = dbscan_merge_to_single_box(boxes_np, eps_frac=0.05)\n",
    "    merged_t = torch.from_numpy(merged_np)\n",
    "\n",
    "    # ---- Annotate merged box\n",
    "    annot_img = annotate(image_source, merged_t, logits=torch.ones((1,)), phrases=[TEXT_PROMPT])\n",
    "    annot_path = os.path.join(OUTDIR, f\"{run_tag}_annot.png\")\n",
    "    cv2.imwrite(annot_path, cv2.cvtColor(annot_img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "    # ---- SAM segmentation\n",
    "    image_bgr = cv2.cvtColor(image_source, cv2.COLOR_RGB2BGR)\n",
    "    masks, _, _ = sam_predictor.predict(box=merged_np[0], multimask_output=False)\n",
    "    mask_img = image_bgr.copy()\n",
    "    mask_img[masks[0]] = (0, 255, 0)  # green overlay\n",
    "    mask_path = os.path.join(OUTDIR, f\"{run_tag}_mask.png\")\n",
    "    cv2.imwrite(mask_path, mask_img)\n",
    "\n",
    "    # ---- CLIP scoring\n",
    "    score = clip_similarity_score(annot_path, TEXT_PROMPT)\n",
    "    results.append({\"score\": score, \"pair\": (box_th, text_th), \"annot_path\": annot_path, \"mask_path\": mask_path})\n",
    "    print(f\"[{run_tag}] CLIP score = {score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596db1a6-7141-40f7-a072-0da9694a74e3",
   "metadata": {},
   "source": [
    "## ðŸ Final Stage: Select Best Result & Annotate Original Image\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ¥‡ Step 1: Pick Best by CLIP Score\n",
    "- Use `max()` to select the result with the highest CLIP score.\n",
    "- Print summary:\n",
    "  - Best thresholds: `box_th`, `text_th`\n",
    "  - Best CLIP score\n",
    "  - Paths to annotated and mask images\n",
    "\n",
    "---\n",
    "\n",
    "### ðŸ“ Step 2: Copy Best Outputs\n",
    "- Copy best annotated image to:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3c23f7f4-c3fb-41cd-a03a-5fb0a98d859a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================\n",
      "Best thresholds: box_th=0.25, text_th=0.50\n",
      "Best CLIP score: 0.2021\n",
      "Annotated: threshold_runs/run_2_b0.25_t0.50_annot.png\n",
      "Mask:      threshold_runs/run_2_b0.25_t0.50_mask.png\n",
      "\n",
      "Annotating original image with the best result...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved final annotation by overwriting: 'test.png'\n",
      "\n",
      "âœ… Done! All runs saved in 'threshold_runs/'. Best copies: 'best_annotated.png', 'best_mask.png'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n"
     ]
    }
   ],
   "source": [
    "# ---- Pick best by CLIP score\n",
    "best = max(results, key=lambda r: r[\"score\"])\n",
    "print(\"\\n======================\")\n",
    "print(f\"Best thresholds: box_th={best['pair'][0]:.2f}, text_th={best['pair'][1]:.2f}\")\n",
    "print(f\"Best CLIP score: {best['score']:.4f}\")\n",
    "print(f\"Annotated: {best['annot_path']}\")\n",
    "print(f\"Mask:      {best['mask_path']}\")\n",
    "\n",
    "# Copy best to top-level for convenience\n",
    "if best[\"annot_path\"]:\n",
    "    shutil.copy(best[\"annot_path\"], \"best_annotated.png\")\n",
    "if best[\"mask_path\"]:\n",
    "    shutil.copy(best[\"mask_path\"], \"best_mask.png\")\n",
    "\n",
    "    # =================================================================================\n",
    "# ---- ADDED: Annotate the original image with the best bounding box found ----\n",
    "if best[\"score\"] > -1.0:\n",
    "    print(\"\\nAnnotating original image with the best result...\")\n",
    "    \n",
    "    # Re-run detection and merging with the best thresholds to get the final box\n",
    "    best_box_th, best_text_th = best['pair']\n",
    "    final_boxes_t, _, _ = predict(\n",
    "        model=dino_model,\n",
    "        image=image,\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=best_box_th,\n",
    "        text_threshold=best_text_th,\n",
    "        device=DEVICE\n",
    "    )\n",
    "    \n",
    "    if len(final_boxes_t) > 0:\n",
    "        final_boxes_np = final_boxes_t.cpu().numpy()\n",
    "        final_merged_np = dbscan_merge_to_single_box(final_boxes_np)\n",
    "        final_merged_t = torch.from_numpy(final_merged_np)\n",
    "\n",
    "        # Use the original image_source for annotation\n",
    "        final_annotated_img = annotate(\n",
    "            image_source=image_source.copy(), \n",
    "            boxes=final_merged_t, \n",
    "            logits=torch.ones(final_merged_t.shape[0]), \n",
    "            phrases=[TEXT_PROMPT]\n",
    "        )\n",
    "        \n",
    "        # Save the final annotated original image by overwriting it\n",
    "        final_output_path = IMAGE_PATH\n",
    "        cv2.imwrite(final_output_path, cv2.cvtColor(final_annotated_img, cv2.COLOR_RGB2BGR))  # Fixed!\n",
    "        print(f\"Saved final annotation by overwriting: '{final_output_path}'\")\n",
    "    else:\n",
    "        print(\"Could not generate final annotation as no boxes were detected with best thresholds.\")\n",
    "\n",
    "\n",
    "print(\"\\nâœ… Done! All runs saved in 'threshold_runs/'. Best copies: 'best_annotated.png', 'best_mask.png'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c3b5c-e3a8-4838-9fab-898f2bc33a13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
